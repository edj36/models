{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucci Overtime Challenge\n",
    "\n",
    "When a professional (and sometimes college) hockey game is tied after regulation play and goes to overtime, Buccigross will post a tweet on his twitter account (@buccigross) using hashtag \\#bucciovertimechallenge.\n",
    "\n",
    "To participate you tweet #bucciovertimechallenge followed by the two hockey players (one from each team) who you think is most likely to score the game-winning goal for their respective team in overtime. \n",
    "\n",
    "Winners are chosen at random from the pool of participants who selected the correct player. Winners get sent a free t-shirt or something from Buccigross' [site](https://www.bucciot.com).\n",
    "\n",
    "**Idea**: This challenge brings up a [wisdom of the crowds](https://en.wikipedia.org/wiki/Wisdom_of_the_crowd) scenario - the collective wisdom of hockey fans might be better at predicting the game winner of a hockey game than a single professional. I am going to investigate this idea in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import base64\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://benalexkeen.com/interacting-with-the-twitter-api-using-python/\n",
    "\n",
    "https://stackoverflow.com/questions/33308634/how-to-perform-oauth-when-doing-twitter-scraping-with-python-requests\n",
    "\n",
    "https://www.kevinsidwar.com/iot/2017/7/1/the-undocumented-nhl-stats-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_twitter_api_token(api_key, api_secret):\n",
    "    \"\"\"\n",
    "    Authenticates with Twitter API.\n",
    "    \n",
    "    :param api_key:\n",
    "    :param api_secret:\n",
    "    :return: token to use with API requests\n",
    "    \"\"\"\n",
    "    key_secret = '{}:{}'.format(api_key, api_secret).encode('ascii')\n",
    "    b64_encoded_key = base64.b64encode(key_secret).decode('ascii')\n",
    "\n",
    "    base_url = 'https://api.twitter.com'\n",
    "    auth_url = '{}/oauth2/token'.format(base_url)\n",
    "    \n",
    "    auth_url = 'https://api.twitter.com/oauth2/token'\n",
    "\n",
    "    auth_headers = {\n",
    "        'Authorization': 'Basic {}'.format(b64_encoded_key),\n",
    "        'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8'\n",
    "    }\n",
    "\n",
    "    auth_data = {\n",
    "        'grant_type': 'client_credentials'\n",
    "    }\n",
    "\n",
    "    auth_resp = requests.post(auth_url, headers=auth_headers, data=auth_data)\n",
    "    auth_resp.raise_for_status()\n",
    "    return auth_resp.json()['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('config/twitter_creds.json') as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "base_url = 'https://api.twitter.com'\n",
    "auth_token = get_twitter_api_token(creds[\"API_KEY\"], creds[\"API_SECRET\"])\n",
    "\n",
    "search_headers = {\n",
    "    'Authorization': 'Bearer {}'.format(auth_token)    \n",
    "}\n",
    "search_params = {\n",
    "    'q': '#bucciovertimechallenge',\n",
    "    'result_type': 'recent',\n",
    "    'count' : 100\n",
    "}\n",
    "\n",
    "search_url = '{}/1.1/search/tweets.json'.format(base_url)\n",
    "\n",
    "aggregated_results = []\n",
    "\n",
    "search_resp = requests.get(search_url, headers=search_headers, params=search_params)\n",
    "search_resp.raise_for_status()\n",
    "\n",
    "for s in search_resp.json()['statuses']:\n",
    "    aggregated_results.append({\n",
    "        \"screen_name\" : s.get('user').get('screen_name'),\n",
    "        \"created_at\" : s.get('created_at'),\n",
    "        \"text\" : s.get('text')\n",
    "    })\n",
    "\n",
    "next_results = search_resp.json()['search_metadata'].get('next_results')\n",
    "\n",
    "count = 1\n",
    "while next_results is not None:\n",
    "    new_search_url = '{}/1.1/search/tweets.json{}'.format(base_url,next_results)\n",
    "    new_search_resp = requests.get(new_search_url, headers=search_headers)\n",
    "    new_search_resp.raise_for_status()\n",
    "    count += 1\n",
    "    if count % 10 == 0:\n",
    "        time.sleep(0.5)\n",
    "        print(count)\n",
    "    for s in new_search_resp.json()['statuses']:\n",
    "        aggregated_results.append({\n",
    "            \"screen_name\" : s.get('user').get('screen_name'),\n",
    "            \"created_at\" : s.get('created_at'),\n",
    "            \"text\" : s.get('text')\n",
    "        })\n",
    "    next_results = new_search_resp.json()['search_metadata'].get('next_results')\n",
    "\n",
    "print('total requests: {}'.format(count))\n",
    "print('total tweets retrieved: {}'.format(len(aggregated_results)))\n",
    "\n",
    "# need to get around rate limits some how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "with open('data/raw/tweets-{}.json'.format(now), 'w') as outfile:\n",
    "    json.dump(aggregated_results, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "- figure out when (time of day) the gwg goal was scored and by who from nhl api\n",
    "- filter out any tweet that happened after the gwg goal was scored\n",
    "- somehow use python collections.Counter object to count all the words in all the tweets and then get the counts for players (given the roster for each team)\n",
    "- show/plot the data on the guesses and compare to who scored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Counter below is greedy, not necessarily the best way to do this (because we end up counting too many random words that were in the many tweets). Can to think about how to make this better (only count players names, etc.), but this may be harder than its worth..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now = '2019-04-26-22-21-19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/tweets-{}.json'.format(now)) as f:\n",
    "    aggregated = json.load(f)\n",
    "    \n",
    "counts = Counter()\n",
    "for tweet in tqdm(aggregated):\n",
    "    t = tweet['text']\n",
    "    # things to account for:\n",
    "    #  /\n",
    "    # <name>.\n",
    "    # same last name (hard to do)\n",
    "    # time of tweet (can't tweet after goal was scored)\n",
    "    # make sure tweets were from the day of the game, make sure you don't miss if game went past midnight also \n",
    "    # random punctionation markers, replace all with / and then replace / with space\n",
    "    tweet_time  = datetime.strptime(tweet['created_at'], \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "    \n",
    "    difference = datetime.utcnow() - tweet_time\n",
    "\n",
    "    if difference.seconds > 60*60*5: # tweet older than a day\n",
    "        continue\n",
    "    \n",
    "    cleaned = t.lower().replace('.', ' ').replace('/',' ').split()\n",
    "    # periods, colons\n",
    "    counts.update(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to cut down our data for one file per game (since we scraped tweets multiple times during overtime periods). \n",
    "\n",
    "Other option is to run the analysis based on date, hard to do, and then just use whatever file per game that was the closest. Its hard to tell which file corresponds to which game because tweets don't specify a corresponding game, need to look it up based on player name -> team they are on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/2019-nhl-ot-goals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df[\"Date\"], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each day there was an overtime goal, get the files from that day (via glob)\n",
    "# get the file with the most records\n",
    "# get the records into a python counter, the see what \"place\" the goal scorer was in,\n",
    "# relative to number of votes with naive filter, then from there clean things up\n",
    "for i,r in df.iterrows():\n",
    "    dt = r['datetime']\n",
    "    dt_s = dt.strftime(\"%Y-%m-%d\")\n",
    "    found = glob.glob(f'data/raw/tweets-{dt_s}*.txt', recursive=True)\n",
    "with open('{}.json'):\n",
    "    print(r['datetime'])\n",
    "    print(type(r['datetime']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "- https://statsapi.web.nhl.com/api/v1/teams\n",
    "- https://github.com/erunion/sport-api-specifications/tree/master/nhl\n",
    "- https://gitlab.com/dword4/nhlapi/blob/master/stats-api.md\n",
    "- http://www.nhl.com/scores/htmlreports/20182019/GS030221.HTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First we can get the overtime results of every game in the 2019 NHL playoffs from [here](https://www.hockey-reference.com/playoffs/NHL_2019.html#all_ots) by copying the table and saving it as a CSV locally. I have this saved in the `/data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some EDA, I found that the #bucciovertimechallenge tweets did not do very well in predicting winners or scorers for the overtime games. Not the exciting outcome we might hope for, but useful to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
